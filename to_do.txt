Automatisierung:
wie schaffe ich es dass nur bei BERT auf Layers geachtet wird? Weil bei Fasttext sollte das egal sein sonst
wird es viele unnötige Experimente geben... 2 forschleifen ineinander
Zeit logging?! start = time(), end = time() total= end-start
Ergebnisse auswerten
code überall kommentieren


Optional später:
Roberta und AlBERt: optional falls am Ende noch Zeit ist!!!!!!!
-Vokabular for cleaning
-Get Embeddings
tf-idf weighted methode zur berechnung der titles siehe Study project? Optional

Erledigte Sachen:

vocabulary cleaning so einbauen dass es für den jeweiligen Algorithmus das tut:
    noch eine if bedingung: falls vocab in [cleaningsteps] ist dann das geladenen model durch vocab laufen
    lassen...
    Problem: Laufzeit Effizienz? Listen mit den jeweils auszuschließenden Worten erstellen (für keywords und
    comments zsm) dann genauso wie stopwords entfernen
    also dict mit den listen von zu removendem vocab
    -> so kann ich leicht berechnen wieviele worte durch preprocessing entfernt wurden! einfach len(stopwords)
    + len(out_of_vocab)
    Sollte ich sonst die fertigen datensätze vom model abhängig abspeichern? DONE

comments basic glove falsch, das war wohl word2vec, nochmal laufen lassen! DONE!!!

BERT funktionalität einbauen!! Done
model               Done
cleaning: vocab teil     Done
get embeddings      Done
experimentor        Done

word_embeddings mit vocabulary cleaning laufen lassen Done

BERT:
- verschiedene Layer Optionen einbauen Done
- Layer 1 bis 12, sentence embeddings? Concat und summing? Done

bei BERT und co: einmal embeddings berechnen für alle Layer? und dann nur nachschlagen welche ich
brauch für jeweiliges Experiment?Done
Oder immer nur das berechnen was ich für Experiment brauche?
Switch Case Schaltung!!! Gibt es nicht...Done